{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbzeXa87qPejNHyJAJrt0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siy0h/AI-Projects/blob/main/PDF_RAG_Chatbot_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Based QA RAG Chatbot\n",
        "This chatbot leverages LangChain, Streamlit and the OpenAI's ChatGPT API to implement a RAG system with the following features:\n",
        "\n",
        "- Web based UI\n",
        "- PDF upload and indexing\n",
        "- RAG System for analyzing and responding to queriews\n",
        "- Real time output\n",
        "- Show document sources from answer of the RAG system"
      ],
      "metadata": {
        "id": "vDgoD-qtT4wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install App and Dependencies"
      ],
      "metadata": {
        "id": "H58JdntIWEyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nETbMFB3Cg6w",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.12\n",
        "!pip install langchain-openai==0.0.8\n",
        "!pip install langchain-community==0.0.29\n",
        "!pip install streamlit==1.32.2\n",
        "!pip install PyMuPDF==1.24.0 #to read pdf\n",
        "!pip install chromadb==0.4.24 #to store embedding\n",
        "!pip install pyngrok==7.1.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Hwpt1BpSDjX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open('/openai_credentials.yml', 'r') as file:\n",
        "    credentials = yaml.safe_load(file)"
      ],
      "metadata": {
        "id": "gyJ9PtQUrxlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgPmgcmor_cX",
        "outputId": "7ea21c86-836c-4b41-e671-445fca8edc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['openai_key'])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = credentials['openai_key']"
      ],
      "metadata": {
        "id": "sdHB9CvgtBbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings #takes chunks and turns them into embeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter #used to split document into smaller chunks\n",
        "from langchain_community.vectorstores import Chroma #vector DB\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "import streamlit as st\n",
        "from operator import itemgetter\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "#initial app landing page\n",
        "st.set_page_config(page_title=\"PDF Chatbot\", page_icon=\":robot:\")\n",
        "st.title(\"Welcome to File QA RAG Chatbot :robot:\")\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "\n",
        "# Takes uploaded PDF documents and extracts the texts\n",
        "# process and break the text down into smaller chunks\n",
        "# Passes chunks through an LLM embedder model and create embeddings\n",
        "# Store document chunks and embeddings into chroma vector database\n",
        "def configure_retriever(uploaded_files):\n",
        "    # Read documents\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploaded_files:\n",
        "      #stores uploaded file in a temporary directory in the server\n",
        "      temp_file_path = os.path.join(temp_dir.name, file.name)\n",
        "      with open(temp_file_path, \"wb\") as f:\n",
        "        #takes PDF documents and extracts the text\n",
        "        f.write(uploaded_files[0].getvalue())\n",
        "    loader = PyMuPDFLoader(temp_file_path)\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "    #break down the documents into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    #create an embedding and store in the Vector DB\n",
        "    embeddings_model = OpenAIEmbeddings()\n",
        "    vectorstore = Chroma.from_documents(chunks, embeddings_model)\n",
        "\n",
        "    #create a retriever object to use later in the RAG process for querying\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    return retriever\n",
        "\n",
        "\n",
        "# Streams the results in real time as it gets responses from ChatGPT\n",
        "# Shows each token on a UI interface\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text: str = \"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "\n",
        "    # creates sidebar to accept PDF uploads\n",
        "\n",
        "    uploaded_files = st.sidebar.file_uploader(\n",
        "        label=\"Upload PDF files\", type=[\"pdf\"], accept_multiple_files=True\n",
        "    )\n",
        "    if not uploaded_files:\n",
        "        st.info(\"Please upload PDF documents to continue.\")\n",
        "        st.stop()\n",
        "\n",
        "    retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "\n",
        "    # create a connection ot ChatGPT LLM\n",
        "    chatgpt = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1,\n",
        "                         streaming = True)\n",
        "\n",
        "\n",
        "\n",
        "    # create a prompt template for QA RAG System -> prompt formulated to make sure that\n",
        "    # the model only answers questions relevant to the retrived documents\n",
        "    qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "    If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "    Use three sentences maximum and keep the answer as concise as possible.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    Helpful Answer:\"\"\"\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "    # formats documents before sending to the LLM. Puts two new lines between every retrieved document\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# QA RAG System Chain\n",
        "\n",
        "# Helps set up input questions and retrieve the relevant contextual docs which\n",
        "# will be used by ChatGPT to answer the question\n",
        "        qa_rag_chain = (\n",
        "        {\"context\": itemgetter(\"question\") #based on the question, use the retriever to find the closest documents context doc\n",
        "         |\n",
        "         retriever\n",
        "         | format_docs, #puts each retieved document chunks between two lines\n",
        "         \"question\": itemgetter(\"question\") #user question\n",
        "         }\n",
        "        | qa_prompt #sends the above info to populate the prompt\n",
        "        | chatgpt #sends the above prompt to chatgpt to generate an answer\n",
        "    )\n",
        "\n",
        "        streamlit_msg_history = StreamlitChatMessageHistory(key = \"langchain_messages\")\n",
        "        if len(streamlit_msg_history.messages) == 0:\n",
        "            streamlit_msg_history.add_ai_message(\"How can I help you?\")\n",
        "\n",
        "\n",
        "        for msg in streamlit_msg_history.messages:\n",
        "            st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "        class PostMessageHandler(BaseCallbackHandler):\n",
        "            def __init__(self, container, sources):\n",
        "                self.container = container\n",
        "                self.sources = []\n",
        "\n",
        "\n",
        "            def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
        "\n",
        "              source_ids = []\n",
        "              for d in documents:\n",
        "                metadata = {\n",
        "                    \"source\": d.metadata['source'],\n",
        "                    \"page\": d.metadata['page'],\n",
        "                    \"content\": d.page_content[:200]\n",
        "                }\n",
        "\n",
        "\n",
        "                #stores in the empty list only if the source is unique\n",
        "                idx = (metadata[\"source\"], metadata[\"page\"])\n",
        "                if idx not in source_ids:\n",
        "                  source_ids.append(idx)\n",
        "                  self.sources.append(metadata)\n",
        "\n",
        "            def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "              if len(self.sources)> 0 :\n",
        "                st.markdown(\"__Sources__\" + \"\\n\")\n",
        "                st.dataframe(data = pd.DataFrame(self.sources[:3]), width = 1000)\n",
        "\n",
        "        if user_prompt := st.chat_input():\n",
        "            st.chat_message(\"human\").write(user_prompt)\n",
        "\n",
        "            with st.chat_message(\"ai\"):\n",
        "                stream_handler = StreamHandler(st.empty())\n",
        "                #tells us where the sources should be displayed\n",
        "\n",
        "                sources_contrainer = st.write(\"\")\n",
        "                pm_handler = PostMessageHandler(sources_contrainer)\n",
        "\n",
        "                config = {\"callbacks\":[stream_handler,pm_handler]}\n",
        "\n",
        "                response = qa_rag_chain.invoke({\"question\":user_prompt},config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOHxuJcMtMmj",
        "outputId": "8f182231-d0f4-470a-ad52-85a29c4d6e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BBKnUSZrT3bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "DMYKE9mctMYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "with open('/content/ngrok_credentials.yml', 'r') as file:\n",
        "    ngrok_credentials = yaml.safe_load(file)\n",
        "\n",
        "ngrok_credentials.keys()\n",
        "ngrok_auth_token = ngrok_credentials['authtoken']\n",
        "\n",
        "ngrok.set_auth_token(ngrok_auth_token)\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9pUnoVd_jS1",
        "outputId": "633fb7bd-a308-45d8-f263-e24ca4617cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Streamlit App: https://64dc-35-185-158-175.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}